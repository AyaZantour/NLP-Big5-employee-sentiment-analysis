# diagnostic.py
import os
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

print("ğŸ” Diagnostic du modÃ¨le fine-tuned")
print("=" * 60)

# ============ CORRECTION DES CHEMINS ============
# Va un dossier en arriÃ¨re (..) pour trouver models/
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
MODEL_PATH = os.path.join(BASE_DIR, 'models', 'transformers_model')
TOKENIZER_PATH = os.path.join(BASE_DIR, 'models', 'transformers_tokenizer')

print(f"ğŸ“ Base directory: {BASE_DIR}")
print(f"ğŸ“ Model path: {MODEL_PATH}")
print(f"ğŸ“ Tokenizer path: {TOKENIZER_PATH}")

# VÃ©rifie que les fichiers existent
print("\nğŸ” VÃ©rification des fichiers:")
for path, name in [(MODEL_PATH, "ModÃ¨le"), (TOKENIZER_PATH, "Tokenizer")]:
    if os.path.exists(path):
        print(f"âœ… {name}: {path}")
        # Liste les fichiers
        files = os.listdir(path)
        print(f"   Fichiers: {len(files)} fichiers")
        for f in files[:3]:  # Montre les 3 premiers
            print(f"   - {f}")
    else:
        print(f"âŒ {name}: NON TROUVÃ‰ Ã  {path}")

# Charge seulement si les fichiers existent
if os.path.exists(MODEL_PATH) and os.path.exists(TOKENIZER_PATH):
    try:
        tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, local_files_only=True)
        model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, local_files_only=True)
        
        print(f"\nâœ… ModÃ¨le chargÃ©!")
        print(f"ğŸ“‹ Labels configurÃ©s: {model.config.id2label}")
        
        # Test avec des phrases TRÃˆS claires
        test_cases = [
            ("everything is perfect i love it here", "should be POSITIF"),
            ("its horrible,  toxic place", "Devrait Ãªtre NÃ‰GATIF"),
            ("nothing special", "Devrait Ãªtre NEUTRE"),
            ("Je veux dÃ©missionner tellement c'est mauvais", "Devrait Ãªtre NÃ‰GATIF"),
            ("Meilleure entreprise de ma vie", "Devrait Ãªtre POSITIF"),
        ]
        
        print("\nğŸ§ª Tests:")
        for text, expected in test_cases:
            inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=128)
            
            with torch.no_grad():
                outputs = model(**inputs)
                probabilities = torch.softmax(outputs.logits, dim=1)[0]
                prediction = torch.argmax(outputs.logits, dim=1).item()
            
            predicted_label = model.config.id2label[prediction]
            
            print(f"\nğŸ“ '{text[:50]}...'")
            print(f"   Attendu: {expected}")
            print(f"   PrÃ©dit: {predicted_label}")
            print(f"   Confiance: N={probabilities[0]:.1%}, Neu={probabilities[1]:.1%}, P={probabilities[2]:.1%}")
            
            # Analyse
            if "POSITIF" in expected and "Negative" in predicted_label:
                print("   âš ï¸  PROBLÃˆME: Inverse!")
            elif "NÃ‰GATIF" in expected and "Positive" in predicted_label:
                print("   âš ï¸  PROBLÃˆME: Inverse!")
                
    except Exception as e:
        print(f"âŒ Erreur chargement: {e}")
else:
    print("\nâŒ Fichiers manquants. Structure actuelle:")
    base = os.path.dirname(BASE_DIR)
    for root, dirs, files in os.walk(base):
        level = root.replace(base, '').count(os.sep)
        indent = ' ' * 2 * level
        print(f'{indent}ğŸ“‚ {os.path.basename(root)}/')
        subindent = ' ' * 2 * (level + 1)
        for file in files[:3]:
            if file.endswith(('.json', '.pkl', '.bin', '.safetensors')):
                print(f'{subindent}ğŸ“„ {file}')